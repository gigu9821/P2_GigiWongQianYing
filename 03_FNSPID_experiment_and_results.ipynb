{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FNSPID Experiment**"
      ],
      "metadata": {
        "id": "Kk3ukid3J5HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1. Imports, Drive Mount, Paths, and Runtime Settings**"
      ],
      "metadata": {
        "id": "2TmhIDrxJ27z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E4cOdGZ4zBv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from google import genai\n",
        "\n",
        "import math   # Added to avoid missing import\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Google Drive mount failed: {e}\")\n",
        "\n",
        "BASE_PATH = \"/content/drive/My Drive/P2/\"\n",
        "NEWS_DATA_PATH = BASE_PATH + \"djia_news_cleaned_no_duplicates.csv\"\n",
        "SECTOR_PATH = BASE_PATH + \"final_ticker_sector3.csv\"\n",
        "READY_DATA_PATH = BASE_PATH + \"djia_news_ready_for_sentiment.csv\"\n",
        "SAMPLE_OUTPUT_PATH = BASE_PATH + \"fnspid_sample_multi_prompt_v2.csv\"\n",
        "FULL_OUTPUT_FILE = BASE_PATH + \"fnspid_full_sentiment_results.csv\"\n",
        "\n",
        "BATCH_SIZE = 1000   # Number of news rows per batch\n",
        "MAX_WORKERS = 3     # Number of concurrent threads\n",
        "\n",
        "api_key = input(\"Enter your Google GENAI API Key: \").strip()\n",
        "client = genai.Client(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2. Selected Prompts for FNSPID**"
      ],
      "metadata": {
        "id": "6GriXJKAKBxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECTED_PROMPTS = {\n",
        "    \"ZS-3\": \"\"\"Act as a sentiment analysis model trained on financial news headlines.\n",
        "Classify the sentiment of the headline: \"{headline}\".\n",
        "Constraint: Answer with exactly one word: Positive, Negative, or Neutral.\"\"\",\n",
        "\n",
        "    \"RP-3\": \"\"\"Act as a financial expert. Classify the sentiment for {target} based only on the headline \"{headline}\".\n",
        "Constraint: Answer with exactly one word: Positive, Negative, or Neutral.\"\"\",\n",
        "\n",
        "    \"CoT-1\": \"\"\"I will offer you a news headline regarding {target}: \"{headline}\".\n",
        "Please think step by step:\n",
        "1. Analyze the rationale and potential impact on the stock price.\n",
        "2. Identify the sentiment.\n",
        "Constraint: Return your response in this specific format: Rationale: [Your reasoning] Sentiment: [Positive, Negative, or Neutral]\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "u_GBaMx_KAcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3. Load News Data and Inject Sector Metadata**"
      ],
      "metadata": {
        "id": "h_pPkHOfKHAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "print(\"\\nReading input data...\")\n",
        "df_news = pd.read_csv(NEWS_DATA_PATH)\n",
        "df_sector = pd.read_csv(SECTOR_PATH)\n",
        "\n",
        "# Normalize tickers\n",
        "df_news['ticker'] = df_news['ticker'].astype(str).str.strip().str.upper()\n",
        "df_sector['ticker'] = df_sector['ticker'].astype(str).str.strip().str.upper()\n",
        "\n",
        "# Merge sector information\n",
        "print(\"Merging sector information...\")\n",
        "df_merged = df_news.merge(df_sector, on='ticker', how='left')\n",
        "df_merged['sector'] = df_merged['sector'].fillna('General')\n",
        "\n",
        "# Manual sector patch for known DJIA tickers\n",
        "manual_sector_map = {\n",
        "    \"AMGN\": \"Healthcare\",\n",
        "    \"CVX\": \"Energy\",\n",
        "    \"DIS\": \"Communication Services\",\n",
        "    \"DOW\": \"Materials\",\n",
        "    \"IBM\": \"Technology\",\n",
        "    \"MMM\": \"Industrials\",\n",
        "    \"MRK\": \"Healthcare\",\n",
        "    \"TRV\": \"Financials\",\n",
        "    \"WBA\": \"Consumer Staples\"\n",
        "}\n",
        "\n",
        "mask = df_merged['ticker'].isin(manual_sector_map)\n",
        "df_merged.loc[mask, 'sector'] = df_merged.loc[mask, 'ticker'].map(manual_sector_map)\n",
        "\n",
        "print(\"\\nSector distribution (Top 10):\")\n",
        "print(df_merged['sector'].value_counts().head(10))\n",
        "print(f\"Remaining 'General' count: {len(df_merged[df_merged['sector'] == 'General'])}\")\n",
        "\n",
        "# Save the ready-to-run dataset\n",
        "df_merged.to_csv(READY_DATA_PATH, index=False)\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"Preprocessing completed. Saved to:\\n{READY_DATA_PATH}\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "WDLry6iNKIu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4. Sample Run Setup (50 News)**"
      ],
      "metadata": {
        "id": "yDCfjcMMKK7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSampling 50 news rows for prompt testing...\")\n",
        "df_ready = pd.read_csv(READY_DATA_PATH)\n",
        "\n",
        "SAMPLE_SIZE = 50\n",
        "df_sample = df_ready.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "\n",
        "# Preserve original index as news_id\n",
        "if 'news_id' not in df_sample.columns:\n",
        "    df_sample['news_id'] = df_sample.index\n",
        "\n",
        "print(f\"Sample is ready: {len(df_sample)} rows\")"
      ],
      "metadata": {
        "id": "KZVZvHTSKNd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5. Output Parsing: Extract Sentiment Label from Raw Response**"
      ],
      "metadata": {
        "id": "ywsllMmUKQKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentiment_label(raw_text):\n",
        "    if not raw_text:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    txt = str(raw_text).lower().strip()\n",
        "\n",
        "    # Match CoT-style output containing \"Sentiment: ...\"\n",
        "    match = re.search(r\"sentiment[^a-zA-Z]*?(positive|negative|neutral)\", txt)\n",
        "    if match:\n",
        "        return match.group(1).capitalize()\n",
        "\n",
        "    # Fallback: match explicit sentiment words\n",
        "    if re.search(r\"\\bpositive\\b\", txt):\n",
        "        return \"Positive\"\n",
        "    if re.search(r\"\\bnegative\\b\", txt):\n",
        "        return \"Negative\"\n",
        "    if re.search(r\"\\bneutral\\b\", txt):\n",
        "        return \"Neutral\"\n",
        "\n",
        "    return \"Unknown\""
      ],
      "metadata": {
        "id": "XECWhbBzKSRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6. Single Task Inference Function (One News × One Prompt)**"
      ],
      "metadata": {
        "id": "JG4kXvTRKUTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_task(task):\n",
        "    row = task['row']\n",
        "    pid = task['prompt_id']\n",
        "    template = task['template']\n",
        "\n",
        "    headline = str(row['title']) if pd.notna(row['title']) else \"\"\n",
        "    headline = headline.strip()\n",
        "\n",
        "    target = str(row['ticker']).strip()\n",
        "    sector = str(row['sector']).strip()\n",
        "\n",
        "    # Format the prompt safely\n",
        "    try:\n",
        "        prompt_text = template.format(headline=headline, target=target, sector=sector)\n",
        "    except:\n",
        "        prompt_text = f\"{template}\\nHeadline: {headline}\"\n",
        "\n",
        "    # Retry up to 3 times\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                contents=prompt_text\n",
        "            )\n",
        "            try:\n",
        "                raw_text = response.candidates[0].content.parts[0].text\n",
        "            except:\n",
        "                raw_text = response.text\n",
        "\n",
        "            parsed_label = clean_sentiment_label(raw_text)\n",
        "\n",
        "            return {\n",
        "                \"news_id\": row['news_id'],\n",
        "                \"date\": row['date'],\n",
        "                \"ticker\": target,\n",
        "                \"headline\": headline,\n",
        "                \"sector\": sector,\n",
        "                \"prompt_id\": pid,\n",
        "                \"parsed_sentiment\": parsed_label,\n",
        "                \"raw_response\": raw_text,\n",
        "                \"success\": True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            err = str(e).lower()\n",
        "            if \"429\" in err or \"quota\" in err or \"exceed\" in err:\n",
        "                time.sleep(2 * (attempt + 1))\n",
        "            else:\n",
        "                time.sleep(1)\n",
        "\n",
        "    return {\n",
        "        \"news_id\": row['news_id'],\n",
        "        \"prompt_id\": pid,\n",
        "        \"success\": False,\n",
        "        \"error\": \"Failed\"\n",
        "    }"
      ],
      "metadata": {
        "id": "PFW5GzuVKXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 7. Sample Multi-Prompt Execution and Save Output**"
      ],
      "metadata": {
        "id": "Ljv9JFEqKbAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nRunning multi-prompt sample test: each news × {len(SELECTED_PROMPTS)} prompts...\")\n",
        "\n",
        "tasks = []\n",
        "for _, row in df_sample.iterrows():\n",
        "    for pid, template in SELECTED_PROMPTS.items():\n",
        "        tasks.append({\"row\": row, \"prompt_id\": pid, \"template\": template})\n",
        "\n",
        "results = []\n",
        "MAX_WORKERS = 3\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    futures = [executor.submit(process_single_task, t) for t in tasks]\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        results.append(future.result())\n",
        "\n",
        "# Save sample results\n",
        "df_result = pd.DataFrame(results)\n",
        "df_result = df_result.sort_values(by=['news_id', 'prompt_id'])\n",
        "df_result.to_csv(SAMPLE_OUTPUT_PATH, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"Pipeline completed. Results saved to:\\n{SAMPLE_OUTPUT_PATH}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(\"\\nPreview (first 6 rows):\")\n",
        "print(df_result[['news_id', 'ticker', 'prompt_id', 'parsed_sentiment']].head(6))"
      ],
      "metadata": {
        "id": "jNas6ngYKdFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 8. Full Run (Batch Processing + Failure Handling + Resume Logic)**"
      ],
      "metadata": {
        "id": "BeHsJ7orKe9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import datetime\n",
        "\n",
        "print(\"\\n=== Full Run (Production Run with Failure Handling) ===\")\n",
        "\n",
        "LOG_FILE = BASE_PATH + \"run_log.txt\"\n",
        "FAILURE_FILE = BASE_PATH + \"fnspid_failures.csv\"  # Failure record file\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=LOG_FILE,\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "    force=True\n",
        ")\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.WARNING)\n",
        "logging.getLogger('').addHandler(console)\n",
        "\n",
        "# Initialize failure record file\n",
        "if not os.path.exists(FAILURE_FILE):\n",
        "    with open(FAILURE_FILE, 'w', newline='', encoding='utf-8') as f:\n",
        "        csv.writer(f).writerow(['news_id', 'ticker', 'prompt_id', 'error_msg', 'timestamp'])\n",
        "\n",
        "def clean_sentiment_label(raw_text):\n",
        "    if not raw_text:\n",
        "        return \"Unknown\"\n",
        "    txt = str(raw_text).lower().strip()\n",
        "\n",
        "    # Match CoT-style output containing \"Sentiment: ...\"\n",
        "    match = re.search(r\"sentiment[^a-zA-Z]*?(positive|negative|neutral)\", txt)\n",
        "    if match:\n",
        "        return match.group(1).capitalize()\n",
        "\n",
        "    # Fallback: match explicit sentiment words\n",
        "    if re.search(r\"\\bpositive\\b\", txt):\n",
        "        return \"Positive\"\n",
        "    if re.search(r\"\\bnegative\\b\", txt):\n",
        "        return \"Negative\"\n",
        "    if re.search(r\"\\bneutral\\b\", txt):\n",
        "        return \"Neutral\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Load full dataset\n",
        "df_ready = pd.read_csv(READY_DATA_PATH)\n",
        "if 'news_id' not in df_ready.columns:\n",
        "    df_ready['news_id'] = df_ready.index\n",
        "\n",
        "total_news = len(df_ready)\n",
        "num_prompts = len(SELECTED_PROMPTS)\n",
        "TOTAL_TASKS_GLOBAL = total_news * num_prompts\n",
        "num_batches = math.ceil(total_news / BATCH_SIZE)\n",
        "\n",
        "# Initialize main output file\n",
        "if not os.path.exists(FULL_OUTPUT_FILE):\n",
        "    header = ['news_id', 'date', 'ticker', 'sector', 'headline', 'prompt_id', 'parsed_sentiment', 'raw_response', 'success']\n",
        "    with open(FULL_OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
        "        csv.writer(f).writerow(header)\n",
        "\n",
        "def process_task_for_full_run(task):\n",
        "    row = task['row']\n",
        "    pid = task['prompt_id']\n",
        "    template = task['template']\n",
        "\n",
        "    headline = str(row['title']) if pd.notna(row['title']) else \"\"\n",
        "    headline = headline.strip()\n",
        "    target = str(row['ticker']).strip()\n",
        "    sector = str(row['sector']).strip()\n",
        "\n",
        "    try:\n",
        "        prompt = template.format(headline=headline, target=target, sector=sector)\n",
        "    except:\n",
        "        prompt = f\"{template}\\nHeadline: {headline}\"\n",
        "\n",
        "    last_error = \"\"\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            res = client.models.generate_content(model=\"gemini-2.0-flash\", contents=prompt)\n",
        "            try:\n",
        "                raw = res.candidates[0].content.parts[0].text\n",
        "            except:\n",
        "                raw = res.text\n",
        "\n",
        "            label = clean_sentiment_label(raw)\n",
        "            time.sleep(0.05)\n",
        "            return [row['news_id'], row['date'], target, sector, headline, pid, label, raw, True]\n",
        "\n",
        "        except Exception as e:\n",
        "            last_error = str(e)\n",
        "            err_msg = last_error.lower()\n",
        "            if \"429\" in err_msg or \"quota\" in err_msg or \"exceed\" in err_msg:\n",
        "                delay = (2 ** attempt) + random.uniform(0, 1)\n",
        "                if attempt == 0:\n",
        "                    logging.warning(f\"Rate Limit (ID: {row['news_id']})\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                logging.error(f\"API Error (ID: {row['news_id']}): {e}\")\n",
        "                time.sleep(1)\n",
        "\n",
        "    # On failure, append to failure file\n",
        "    try:\n",
        "        with open(FAILURE_FILE, 'a', newline='', encoding='utf-8') as ff:\n",
        "            csv.writer(ff).writerow([\n",
        "                row['news_id'], target, pid, last_error, datetime.datetime.now()\n",
        "            ])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    logging.error(f\"Failed (ID: {row['news_id']}, {pid}): {last_error}\")\n",
        "    return [row['news_id'], row['date'], target, sector, headline, pid, \"Error\", f\"FAILED: {last_error}\", False]\n",
        "\n",
        "# Batch loop with resume logic (only considers success=True as completed)\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * BATCH_SIZE\n",
        "    end_idx = min((batch_idx + 1) * BATCH_SIZE, total_news)\n",
        "\n",
        "    processed_keys = set()\n",
        "    completed_count = 0\n",
        "    if os.path.exists(FULL_OUTPUT_FILE):\n",
        "        try:\n",
        "            df_done = pd.read_csv(FULL_OUTPUT_FILE, usecols=['news_id', 'prompt_id', 'success'])\n",
        "            df_success = df_done[df_done['success'] == True]\n",
        "            processed_keys = set(df_success['news_id'].astype(str) + \"_\" + df_success['prompt_id'])\n",
        "            completed_count = len(processed_keys)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    progress_pct = (completed_count / TOTAL_TASKS_GLOBAL) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Batch {batch_idx + 1}/{num_batches} | News rows: {start_idx}-{end_idx - 1}\")\n",
        "    print(f\"[Valid progress]: {completed_count}/{TOTAL_TASKS_GLOBAL} ({progress_pct:.2f}%)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    df_batch = df_ready.iloc[start_idx:end_idx]\n",
        "    tasks = []\n",
        "    for _, row in df_batch.iterrows():\n",
        "        for pid, template in SELECTED_PROMPTS.items():\n",
        "            task_key = f\"{row['news_id']}_{pid}\"\n",
        "            if task_key not in processed_keys:\n",
        "                tasks.append({\"row\": row, \"prompt_id\": pid, \"template\": template})\n",
        "\n",
        "    if len(tasks) == 0:\n",
        "        print(\"[Skip] All tasks in this batch were already completed successfully.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Pending tasks: {len(tasks)} (including retries for previously failed tasks)\")\n",
        "\n",
        "    with open(FULL_OUTPUT_FILE, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            futures = [executor.submit(process_task_for_full_run, t) for t in tasks]\n",
        "\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {batch_idx+1}\"):\n",
        "                result = future.result()\n",
        "                writer.writerow(result)\n",
        "                f.flush()\n",
        "\n",
        "print(\"\\nRun completed.\")\n",
        "print(f\"Result file: {FULL_OUTPUT_FILE}\")\n",
        "print(f\"Failure file: {FAILURE_FILE} (if any)\")\n",
        "print(f\"Log file: {LOG_FILE}\")"
      ],
      "metadata": {
        "id": "E903x6fVKiXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FNSPID Sentiment Result**"
      ],
      "metadata": {
        "id": "qOw8jMdeKl2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1. Setup, File Paths, and Data Loading**"
      ],
      "metadata": {
        "id": "TbIUepvfKqH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "\n",
        "# 1. Initialization\n",
        "BASE_PATH = \"/content/drive/MyDrive/P2/\"\n",
        "SENTIMENT_FILE = os.path.join(BASE_PATH, \"fnspid_full_sentiment_results.csv\")\n",
        "DJIA_FILE = os.path.join(BASE_PATH, \"DJIA_2021_2023.csv\")\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_BEST_FILE = os.path.join(BASE_PATH, \"Final_Dataset_Predictive_Best_1221.csv\")\n",
        "OUTPUT_LEADERBOARD = os.path.join(BASE_PATH, \"Final_Leaderboard.csv\")\n",
        "\n",
        "# Load files\n",
        "df_sent_raw = pd.read_csv(SENTIMENT_FILE)\n",
        "df_djia_raw = pd.read_csv(DJIA_FILE)"
      ],
      "metadata": {
        "id": "Lxui0zsoLnUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2. Preprocessing: Dates, Sentiment Scoring, and Returns**"
      ],
      "metadata": {
        "id": "z8gJFW85Lpqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: standardize date columns\n",
        "df_sent_raw['date'] = pd.to_datetime(df_sent_raw['date'], utc=True).dt.date\n",
        "df_djia_raw['date'] = pd.to_datetime(df_djia_raw['date']).dt.date\n",
        "\n",
        "# Map sentiment labels to numeric scores\n",
        "sentiment_map = {'Positive': 1, 'Negative': -1, 'Neutral': 0}\n",
        "df_sent_raw['score'] = df_sent_raw['parsed_sentiment'].map(sentiment_map).fillna(0)\n",
        "\n",
        "# Prepare price data and returns\n",
        "df_price = df_djia_raw.sort_values('date').copy()\n",
        "df_price['Return'] = df_price['close'].pct_change()\n",
        "df_price['Next_Return'] = df_price['Return'].shift(-1)      # T+1\n",
        "df_price['Next_Return_T2'] = df_price['Return'].shift(-2)   # T+2 (backup)"
      ],
      "metadata": {
        "id": "Ck_kwixWLsJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3. Prompt-Level Evaluation and Leaderboard Construction**"
      ],
      "metadata": {
        "id": "oYW6MFZPLwEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing prompt performance and generating leaderboard...\")\n",
        "\n",
        "all_prompts = df_sent_raw['prompt_id'].unique()\n",
        "table_rows = []  # Store leaderboard rows\n",
        "\n",
        "for pid in all_prompts:\n",
        "    # Filter by prompt_id\n",
        "    sub_df = df_sent_raw[df_sent_raw['prompt_id'] == pid].copy()\n",
        "\n",
        "    # Aggregate to daily counts and sentiment composition\n",
        "    daily = sub_df.groupby('date').agg(\n",
        "        total=('news_id', 'count'),\n",
        "        pos=('score', lambda x: (x == 1).sum()),\n",
        "        neg=('score', lambda x: (x == -1).sum())\n",
        "    ).reset_index()\n",
        "\n",
        "    # Compute DMSI (Daily Market Sentiment Index)\n",
        "    daily['DMSI'] = (daily['pos'] - daily['neg']) / daily['total']\n",
        "\n",
        "    # Merge with price data\n",
        "    merged = pd.merge(df_price, daily, on='date', how='inner').dropna()\n",
        "\n",
        "    if len(merged) > 30:\n",
        "        # Core metric: T+1 IC (correlation between DMSI and next-day return)\n",
        "        ic_next = merged['DMSI'].corr(merged['Next_Return'])\n",
        "\n",
        "        # Store intermediate results for deeper analysis later\n",
        "        row_data = {\n",
        "            'Prompt': pid,\n",
        "            'T+1 IC (Predictive)': ic_next,\n",
        "            'Granger p-value': '-',     # Placeholder (computed only for the best prompt)\n",
        "            'T+2 IC (Safety)': '-',     # Placeholder (computed only for the best prompt)\n",
        "            'Description': 'Baselines', # Will be overwritten below if matched\n",
        "            '_data': merged             # Cache merged data for the best prompt\n",
        "        }\n",
        "\n",
        "        # Assign a simple strategy label based on the prompt id\n",
        "        if 'ZS' in pid:\n",
        "            row_data['Description'] = 'Zero-Shot'\n",
        "        if 'CoT' in pid:\n",
        "            row_data['Description'] = 'Chain-of-Thought'\n",
        "        if 'RP' in pid:\n",
        "            row_data['Description'] = 'Role-Playing'\n",
        "\n",
        "        table_rows.append(row_data)\n",
        "\n",
        "# Build leaderboard table and sort by absolute predictive IC\n",
        "df_table = pd.DataFrame(table_rows)\n",
        "df_table = df_table.sort_values('T+1 IC (Predictive)', key=abs, ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "-9N3tgaVLvkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4. Deep Validation for the Best Prompt: T+2 IC and Granger Test**"
      ],
      "metadata": {
        "id": "pvIB8p3HLzwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning deeper validation for the top-ranked prompt...\")\n",
        "\n",
        "# Extract best prompt row\n",
        "best_row_idx = 0\n",
        "best_data = df_table.loc[best_row_idx, '_data']\n",
        "\n",
        "# Compute T+2 IC (safety check)\n",
        "ic_t2 = best_data['DMSI'].corr(best_data['Next_Return_T2'])\n",
        "df_table.loc[best_row_idx, 'T+2 IC (Safety)'] = f\"{ic_t2:.4f}\"\n",
        "\n",
        "# Compute Granger causality p-value (lag=1)\n",
        "gc_res = grangercausalitytests(best_data[['Return', 'DMSI']], maxlag=1, verbose=False)\n",
        "p_val = gc_res[1][0]['ssr_ftest'][1]\n",
        "p_str = \"< 0.001\" if p_val < 0.001 else f\"{p_val:.4f}\"\n",
        "df_table.loc[best_row_idx, 'Granger p-value'] = p_str\n",
        "\n",
        "# Mark the best prompt for display only\n",
        "df_table.loc[best_row_idx, 'Prompt'] = f\"**{df_table.loc[best_row_idx, 'Prompt']}**\""
      ],
      "metadata": {
        "id": "qQiy-2GjL3ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5. Save Outputs: Best Dataset and Leaderboard**"
      ],
      "metadata": {
        "id": "UdvkHSLjL7Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSaving output files...\")\n",
        "\n",
        "# Save best dataset (used for Streamlit and plotting)\n",
        "best_data.to_csv(OUTPUT_BEST_FILE, index=False)\n",
        "print(f\"Best prompt dataset saved: {OUTPUT_BEST_FILE}\")\n",
        "\n",
        "# Save leaderboard table (drop cached data column)\n",
        "final_display_df = df_table.drop(columns=['_data'])\n",
        "final_display_df.index = final_display_df.index + 1\n",
        "final_display_df.to_csv(OUTPUT_LEADERBOARD, index=True)\n",
        "print(f\"Leaderboard saved: {OUTPUT_LEADERBOARD}\")"
      ],
      "metadata": {
        "id": "yxCF_RExL8So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6. Display and Plot the Top Prompts by Predictive IC**"
      ],
      "metadata": {
        "id": "8ptRCz3LMAfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Leaderboard\")\n",
        "print(\"=\"*60)\n",
        "display(final_display_df)\n",
        "\n",
        "# Plot top 5 prompts by T+1 IC\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_df = df_table.head(5)\n",
        "colors = ['#D62728' if i == 0 else '#A9A9A9' for i in range(len(plot_df))]\n",
        "plt.bar(plot_df['Prompt'].str.replace('**', ''), plot_df['T+1 IC (Predictive)'], color=colors)\n",
        "plt.title('Predictive Power (T+1 IC) Comparison')\n",
        "plt.ylabel('Information Coefficient (IC)')\n",
        "plt.xlabel('Prompt Strategy')\n",
        "plt.axhline(0.05, color='gray', linestyle='--', alpha=0.5, label='Significance Level')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAll steps completed. You can download the generated CSV files from the P2 folder.\")"
      ],
      "metadata": {
        "id": "9No9ZVADMBPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 7. Simple Backtest: Strategy vs Benchmark Cumulative Returns**"
      ],
      "metadata": {
        "id": "vJ90bJLXME-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nRunning a simple cumulative return backtest...\")\n",
        "\n",
        "# Define a simple long/short signal:\n",
        "# Go long when DMSI > 0, go short when DMSI < 0\n",
        "best_data['Signal'] = np.where(best_data['DMSI'] > 0, 1, -1)\n",
        "\n",
        "# Strategy return = signal * next-day market return\n",
        "best_data['Strategy_Return'] = best_data['Signal'] * best_data['Next_Return']\n",
        "\n",
        "# Cumulative returns (compounded)\n",
        "best_data['Cumulative_Strategy'] = (1 + best_data['Strategy_Return']).cumprod()\n",
        "best_data['Cumulative_Benchmark'] = (1 + best_data['Next_Return']).cumprod()\n",
        "\n",
        "# Plot cumulative returns\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(\n",
        "    best_data['date'],\n",
        "    best_data['Cumulative_Strategy'],\n",
        "    label='ZS-3 Sentiment Strategy',\n",
        "    color='#D62728',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "    best_data['date'],\n",
        "    best_data['Cumulative_Benchmark'],\n",
        "    label='Buy & Hold DJIA (Benchmark)',\n",
        "    color='gray',\n",
        "    linestyle='--',\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "plt.title('Cumulative Returns: Sentiment Strategy vs Benchmark', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Normalized Wealth (Start = 1.0)')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Final return multiple\n",
        "total_return = best_data['Cumulative_Strategy'].iloc[-1] - 1\n",
        "print(\"Backtest summary:\")\n",
        "print(f\"   - Strategy total return: {total_return * 100:.2f}%\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-hGqksYpMGFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}