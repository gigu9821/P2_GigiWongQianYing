{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**FNSPID Data**"
      ],
      "metadata": {
        "id": "QjBBqC8e1lh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1. Environment Setup and Authentication"
      ],
      "metadata": {
        "id": "BA7K01sS1pLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqOBfjaw1T-2"
      },
      "outputs": [],
      "source": [
        "#  Install and import required libraries\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install datasets -q\n",
        "!pip install langdetect -q\n",
        "!pip install huggingface_hub -q\n",
        "print(\"Library installation completed.\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from langdetect import detect, LangDetectException\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "#  Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Google Drive mount failed: {e}\")\n",
        "\n",
        "# Hugging Face authentication\n",
        "try:\n",
        "    login(token=userdata.get('HF_TOKEN'))\n",
        "    print(\"Hugging Face token login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Hugging Face login failed. Proceeding anonymously. Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2. Filtering Configuration"
      ],
      "metadata": {
        "id": "5DxCX39O2vWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define filtering conditions\n",
        "print(\"\\nDefining filtering conditions...\")\n",
        "\n",
        "djia_tickers = {\n",
        "    \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \"GS\", \"HD\", \"HON\",\n",
        "    \"IBM\", \"INTC\", \"JNJ\", \"KO\", \"JPM\", \"MCD\", \"MMM\", \"MRK\", \"MSFT\", \"NKE\",\n",
        "    \"PG\", \"TRV\", \"UNH\", \"CRM\", \"VZ\", \"V\", \"WBA\", \"WMT\", \"DIS\", \"DOW\"\n",
        "}\n",
        "\n",
        "start_date_str = \"2021-01-01\"\n",
        "end_date_str = \"2023-12-31\"\n",
        "output_path = \"/content/drive/MyDrive/djia_news_cleaned_2021_2023.csv\"\n",
        "\n",
        "print(f\"Filtering conditions defined. Output file will be saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "gJxt4wUN2uRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3. Streaming Dataset Loading"
      ],
      "metadata": {
        "id": "mywWGd462yzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset in streaming mode\n",
        "print(\"\\nLoading dataset in streaming mode...\")\n",
        "try:\n",
        "    news_stream = load_dataset(\"Zihan1004/FNSPID\", streaming=True)['train']\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Dataset loading failed: {e}\")\n",
        "    news_stream = None\n"
      ],
      "metadata": {
        "id": "j1HwhGF720PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4. One-Pass Streaming Cleaning and Filtering"
      ],
      "metadata": {
        "id": "RYlBnRRG218c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-pass processing for all tasks\n",
        "if news_stream:\n",
        "    print(\"\\n--- Starting single-pass streaming processing ---\")\n",
        "\n",
        "    clean_records = []\n",
        "    found_tickers = set()\n",
        "    skipped_record_count = 0\n",
        "\n",
        "    progress_bar = tqdm(news_stream, desc=\"Processing data stream\")\n",
        "\n",
        "    for record in progress_bar:\n",
        "        try:\n",
        "            # Core modification: use .get() to safely retrieve values inside the loop\n",
        "            date_val = record.get('Date')\n",
        "            ticker_val = record.get('Stock_symbol')\n",
        "            title_val = record.get('Article_title')\n",
        "\n",
        "            # Skip records if any required field is missing or empty\n",
        "            if not date_val or not ticker_val or not title_val:\n",
        "                skipped_record_count += 1\n",
        "                continue\n",
        "\n",
        "            # Apply date range and DJIA ticker filtering\n",
        "            news_date = date_val[:10]\n",
        "            if not (start_date_str <= news_date <= end_date_str and ticker_val in djia_tickers):\n",
        "                continue\n",
        "\n",
        "            found_tickers.add(ticker_val)\n",
        "\n",
        "            # Language detection check (only validating detectability)\n",
        "            try:\n",
        "                detect(title_val)\n",
        "            except LangDetectException:\n",
        "                skipped_record_count += 1\n",
        "                continue\n",
        "\n",
        "            # Append cleaned record with selected fields only\n",
        "            clean_records.append({\n",
        "                'Date': date_val,\n",
        "                'Article_title': title_val,\n",
        "                'Stock_symbol': ticker_val\n",
        "            })\n",
        "\n",
        "            progress_bar.set_description(\n",
        "                f\"Processed records | Found {len(found_tickers)}/{len(djia_tickers)} tickers\"\n",
        "            )\n",
        "\n",
        "        except Exception:\n",
        "            skipped_record_count += 1\n",
        "            continue\n",
        "\n",
        "    print(\"\\nStreaming data processing completed.\")\n",
        "\n",
        "    #  Post-processing and reporting\n",
        "    print(f\"\\nIntegrity check: Found records for {len(found_tickers)}/{len(djia_tickers)} DJIA companies.\")\n",
        "    missing_tickers = djia_tickers - found_tickers\n",
        "\n",
        "    if not missing_tickers:\n",
        "        print(\"All DJIA tickers were found within the specified time range.\")\n",
        "    else:\n",
        "        print(f\"Warning: {len(missing_tickers)} DJIA tickers were missing: {sorted(list(missing_tickers))}\")\n",
        "\n",
        "    if clean_records:\n",
        "        # Create DataFrame with only required columns\n",
        "        df_final_clean_news = pd.DataFrame(clean_records)\n",
        "        df_final_clean_news = df_final_clean_news.rename(\n",
        "            columns={'Date': 'date', 'Article_title': 'title', 'Stock_symbol': 'ticker'}\n",
        "        )\n",
        "        print(\"\\nColumn names have been renamed.\")\n",
        "\n",
        "        print(f\"\\nFinal dataset contains {len(df_final_clean_news)} high-quality news records.\")\n",
        "        print(f\"{skipped_record_count} records were skipped due to missing or invalid data.\")\n",
        "\n",
        "        print(\"\\nPreview of the final dataset:\")\n",
        "        print(df_final_clean_news.head())\n",
        "\n",
        "        print(f\"\\nSaving cleaned dataset to: {output_path} ...\")\n",
        "        df_final_clean_news.to_csv(output_path, index=False)\n",
        "        print(\"Data successfully saved to Google Drive.\")\n",
        "    else:\n",
        "        print(\"\\nNo valid records were found after applying all filters.\")\n",
        "        print(f\"{skipped_record_count} records were skipped due to missing or invalid data.\")\n"
      ],
      "metadata": {
        "id": "ReCLxBFA239P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 5. News Volume Statistics by Ticker"
      ],
      "metadata": {
        "id": "UTcb0cdi28et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Target tickers\n",
        "targets = ['CSCO', 'HD', 'HON', 'JNJ', 'JPM', 'MCD', 'PG', 'UNH', 'VZ']\n",
        "\n",
        "# Counters\n",
        "counts_all = Counter()\n",
        "counts_2021_2023 = Counter()\n",
        "\n",
        "# Date range\n",
        "start_date = datetime(2021, 1, 1)\n",
        "end_date = datetime(2023, 12, 31)\n",
        "\n",
        "# Load FNSPID dataset (no subsets available)\n",
        "news_stream = load_dataset(\n",
        "    \"Zihan1004/FNSPID\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Iterate through streaming data with progress bar\n",
        "for record in tqdm(news_stream, desc=\"Scanning news records\", unit=\"records\"):\n",
        "    ticker = record.get(\"Stock_symbol\")\n",
        "    date_str = record.get(\"Date\")\n",
        "\n",
        "    if not ticker or not date_str:\n",
        "        continue\n",
        "\n",
        "    # Count all records\n",
        "    if ticker in targets:\n",
        "        counts_all[ticker] += 1\n",
        "\n",
        "    # Parse date and filter by range\n",
        "    try:\n",
        "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S %Z\")\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    if ticker in targets and start_date <= date_obj <= end_date:\n",
        "        counts_2021_2023[ticker] += 1\n",
        "\n",
        "# Output comparison results\n",
        "print(\"\\nOverall counts:\")\n",
        "for t in targets:\n",
        "    print(f\"{t}: {counts_all[t]} records\")\n",
        "\n",
        "print(\"\\nCounts for 2021–2023:\")\n",
        "for t in targets:\n",
        "    print(f\"{t}: {counts_2021_2023[t]} records\")"
      ],
      "metadata": {
        "id": "pKPFD2dm3A-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 6. Diagnostic Analysis for Rejected Records"
      ],
      "metadata": {
        "id": "4wbfMhUK3DkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "#Configuration\n",
        "targets_to_investigate = ['CSCO', 'HD', 'HON', 'JNJ', 'JPM', 'MCD', 'PG', 'UNH', 'VZ']\n",
        "max_samples_per_ticker = 5  # Maximum rejected samples per ticker\n",
        "start_date_str = \"2021-01-01\"\n",
        "end_date_str = \"2023-12-31\"\n",
        "\n",
        "#Load streaming dataset\n",
        "news_stream = load_dataset(\"Zihan1004/FNSPID\", streaming=True)['train']\n",
        "\n",
        "#Start diagnostic process\n",
        "if news_stream:\n",
        "    print(\"\\n--- Starting diagnostic analysis for target companies ---\")\n",
        "\n",
        "    found_samples = {ticker: [] for ticker in targets_to_investigate}\n",
        "    tickers_with_enough_samples = set()\n",
        "\n",
        "    progress_bar = tqdm(news_stream, desc=\"Scanning data stream for diagnostics\")\n",
        "\n",
        "    for record in progress_bar:\n",
        "        #Stop early if all target samples are collected\n",
        "        if len(tickers_with_enough_samples) == len(targets_to_investigate):\n",
        "            progress_bar.close()\n",
        "            break\n",
        "\n",
        "        ticker_val = record.get('Stock_symbol')\n",
        "\n",
        "        if ticker_val in targets_to_investigate and len(found_samples[ticker_val]) < max_samples_per_ticker:\n",
        "\n",
        "            rejection_reasons = []  # Store rejection reasons for this record\n",
        "\n",
        "            #Diagnostic 1: Record completeness check\n",
        "            date_val = record.get('Date')\n",
        "            title_val = record.get('Article_title')\n",
        "            if not date_val or not title_val:\n",
        "                rejection_reasons.append(\"Incomplete record (missing date or title)\")\n",
        "\n",
        "            #Diagnostic 2: Date range check\n",
        "            if date_val:\n",
        "                try:\n",
        "                    news_date = date_val[:10]\n",
        "                    if not (start_date_str <= news_date <= end_date_str):\n",
        "                        rejection_reasons.append(f\"Date out of range (date = {news_date})\")\n",
        "                except (TypeError, IndexError):\n",
        "                    rejection_reasons.append(\"Invalid date format\")\n",
        "\n",
        "            #Diagnostic 3: Language check\n",
        "            if title_val:\n",
        "                try:\n",
        "                    if detect(title_val) != 'en':\n",
        "                        rejection_reasons.append(\"Non-English language\")\n",
        "                except LangDetectException:\n",
        "                    rejection_reasons.append(\"Language detection failed\")\n",
        "\n",
        "            #Record rejected samples\n",
        "            if rejection_reasons:\n",
        "                found_samples[ticker_val].append({\n",
        "                    \"record\": {\n",
        "                        \"Date\": date_val,\n",
        "                        \"Stock_symbol\": ticker_val,\n",
        "                        \"Article_title\": title_val\n",
        "                    },\n",
        "                    \"reasons\": rejection_reasons\n",
        "                })\n",
        "\n",
        "                if len(found_samples[ticker_val]) >= max_samples_per_ticker:\n",
        "                    tickers_with_enough_samples.add(ticker_val)\n",
        "\n",
        "    #Print diagnostic results\n",
        "    print(\"\\n--- Diagnostic Results ---\")\n",
        "    for ticker, samples in found_samples.items():\n",
        "        if not samples:\n",
        "            print(f\"\\n{ticker}: No rejected samples found (either no data or all records passed).\")\n",
        "        else:\n",
        "            print(f\"\\n{ticker}: {len(samples)} rejected samples found\")\n",
        "            for s in samples:\n",
        "                rec = s[\"record\"]\n",
        "                reasons = \"; \".join(s[\"reasons\"])\n",
        "                print(f\"  - Date: {rec['Date']} | Title: {rec['Article_title'][:50]}... | Reason: {reasons}\")\n"
      ],
      "metadata": {
        "id": "fw6qiF863C-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DJIA Price Data Download and Preprocessing (2021–2023)**"
      ],
      "metadata": {
        "id": "8SidutCl3G2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Dow Jones Industrial Average (^DJI) price data\n",
        "djia = yf.download(\"^DJI\", start=\"2021-01-01\", end=\"2023-12-31\")\n",
        "\n",
        "# Save the raw price data to Google Drive (MyDrive root directory)\n",
        "save_path = \"/content/drive/MyDrive/DJIA_2021_2023.csv\"\n",
        "djia.to_csv(save_path)\n",
        "\n",
        "print(f\"File successfully saved to {save_path}\")\n",
        "\n",
        "# Define input and output file paths\n",
        "price_path = \"/content/drive/My Drive/DJIA_2021_2023.csv\"\n",
        "output_path = \"/content/drive/My Drive/DJIA_2021_2023.csv\"\n",
        "\n",
        "# Define column names explicitly\n",
        "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "\n",
        "# Read the DJIA CSV file and remove multi-row headers\n",
        "price_df = pd.read_csv(\n",
        "    price_path,\n",
        "    skiprows=3,      # Skip Price / Ticker / Date rows\n",
        "    header=None,\n",
        "    names=columns\n",
        ")\n",
        "\n",
        "# Convert the date column to datetime format\n",
        "price_df[\"date\"] = pd.to_datetime(price_df[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# Remove rows with invalid or missing dates\n",
        "price_df = price_df.dropna(subset=[\"date\"])\n",
        "\n",
        "# Remove duplicate trading days based on the date column\n",
        "price_df = price_df.drop_duplicates(subset=[\"date\"], keep=\"first\")\n",
        "\n",
        "# Sort the data chronologically by date\n",
        "price_df = price_df.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Save the cleaned price data back to Google Drive\n",
        "price_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Cleaning completed\")\n",
        "print(f\"Saved to: {output_path}\")\n",
        "print(f\"Dataset shape: {price_df.shape}\")\n",
        "print(price_df.head())\n"
      ],
      "metadata": {
        "id": "LYFwZ8_33hp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIQA-2018 Balanced Dataset Construction and Deduplication**"
      ],
      "metadata": {
        "id": "Ikgiq-uGA6I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "NEUTRAL_BAND = 0.3\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FINAL_SAVE_PATH = SAVE_DIR / \"fiqa_1.csv\"\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "ds = load_dataset(\"pauri32/fiqa-2018\")\n",
        "\n",
        "df_all = pd.concat(\n",
        "    [\n",
        "        pd.DataFrame(ds[\"train\"]),\n",
        "        pd.DataFrame(ds[\"validation\"]),\n",
        "        pd.DataFrame(ds[\"test\"])\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "print(f\"Total number of records loaded: {df_all.shape[0]}\")\n",
        "\n",
        "def label_from_score(score, neutral_band=NEUTRAL_BAND):\n",
        "    if score > neutral_band:\n",
        "        return \"Positive\"\n",
        "    elif score < -neutral_band:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "df_all[\"label\"] = df_all[\"sentiment_score\"].apply(\n",
        "    lambda s: label_from_score(s, NEUTRAL_BAND)\n",
        ")\n",
        "\n",
        "def make_balanced_sample(df, random_state=RANDOM_STATE):\n",
        "    counts = df[\"label\"].value_counts()\n",
        "    per_class = int(counts.min())\n",
        "\n",
        "    sampled_parts = []\n",
        "    for cls in [\"Positive\", \"Neutral\", \"Negative\"]:\n",
        "        subset = df[df[\"label\"] == cls]\n",
        "        sampled_parts.append(subset.sample(per_class, random_state=random_state))\n",
        "\n",
        "    balanced_df = (\n",
        "        pd.concat(sampled_parts, ignore_index=True)\n",
        "        .sample(frac=1, random_state=random_state)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = make_balanced_sample(df_all)\n",
        "\n",
        "print(\"\\nBalanced dataset label distribution (before deduplication):\")\n",
        "print(balanced_df[\"label\"].value_counts())\n",
        "\n",
        "num_dup_before = balanced_df.duplicated().sum()\n",
        "print(\"\\nDuplicate check before deduplication:\")\n",
        "print(f\"Total rows: {balanced_df.shape[0]}\")\n",
        "print(f\"Number of duplicated rows: {num_dup_before}\")\n",
        "\n",
        "if num_dup_before > 0:\n",
        "    print(\"\\nSample duplicated rows:\")\n",
        "    print(balanced_df[balanced_df.duplicated()].head())\n",
        "\n",
        "balanced_df = balanced_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "num_dup_after = balanced_df.duplicated().sum()\n",
        "print(\"\\nDuplicate check after deduplication:\")\n",
        "print(f\"Total rows: {balanced_df.shape[0]}\")\n",
        "print(f\"Number of duplicated rows: {num_dup_after}\")\n",
        "\n",
        "print(\"\\nBalanced dataset label distribution (after deduplication):\")\n",
        "print(balanced_df[\"label\"].value_counts())\n",
        "\n",
        "balanced_df.to_csv(FINAL_SAVE_PATH, index=False)\n",
        "print(f\"\\nFinal dataset saved to: {FINAL_SAVE_PATH}\")\n"
      ],
      "metadata": {
        "id": "ZVmDEO9i35Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Visualization of FNSPID News, DJIA Prices, and FIQA Sentiment**"
      ],
      "metadata": {
        "id": "oYOKAjxFEdi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load cleaned FNSPID news data\n",
        "news_path = \"/content/drive/MyDrive/djia_news_cleaned_2021_2023.csv\"\n",
        "news_df = pd.read_csv(news_path)\n",
        "\n",
        "# Load DJIA price data\n",
        "djia_path = \"/content/drive/MyDrive/DJIA_2021_2023.csv\"\n",
        "djia_df = pd.read_csv(djia_path)\n",
        "\n",
        "# Load FIQA dataset with sentiment labels\n",
        "fiqa_path = \"/content/drive/MyDrive/fiqa_1.csv\"\n",
        "fiqa_df = pd.read_csv(fiqa_path)\n",
        "\n",
        "# Global plot style settings\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 14,\n",
        "    \"axes.titlesize\": 18,\n",
        "    \"axes.labelsize\": 16,\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "    \"legend.fontsize\": 12\n",
        "})\n",
        "\n",
        "timeline_color = \"#6EC6FF\"\n",
        "\n",
        "# Convert date columns to datetime\n",
        "news_df[\"date\"] = pd.to_datetime(news_df[\"date\"], errors=\"coerce\")\n",
        "djia_df[\"date\"] = pd.to_datetime(djia_df[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# Ensure price column is numeric\n",
        "djia_df[\"close\"] = pd.to_numeric(djia_df[\"close\"], errors=\"coerce\")\n",
        "\n",
        "# Plot daily FNSPID news counts\n",
        "news_counts = (\n",
        "    news_df.dropna(subset=[\"date\"])\n",
        "    .groupby(news_df[\"date\"].dt.date)\n",
        "    .size()\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(11.5, 2.6))\n",
        "plt.plot(news_counts.index, news_counts.values, linewidth=1.5, color=timeline_color)\n",
        "plt.title(\"FNSPID News: Daily News Counts (2021–2023)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of News\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/fnspid_news_distribution.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot DJIA closing price trend\n",
        "djia_plot = djia_df.dropna(subset=[\"date\", \"close\"]).sort_values(\"date\")\n",
        "\n",
        "plt.figure(figsize=(11.5, 2.6))\n",
        "plt.plot(djia_plot[\"date\"], djia_plot[\"close\"], linewidth=1.5, color=timeline_color)\n",
        "plt.title(\"DJIA Historical Prices: Closing Price Trend (2021–2023)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/djia_close_trend.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot FIQA sentiment label distribution\n",
        "label_order = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "label_counts = fiqa_df[\"label\"].value_counts().reindex(label_order).fillna(0)\n",
        "\n",
        "colors = [\"#A8E6CF\", \"#A0CED9\", \"#FFB7B2\"]\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.pie(\n",
        "    label_counts,\n",
        "    labels=label_counts.index,\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=90,\n",
        "    colors=colors\n",
        ")\n",
        "plt.title(\"FIQA-2018 Sentiment Label Distribution\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/fiqa_label_distribution.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "#FNSPID News Count by Ticker (Top 10)\n",
        "# Count number of news per ticker\n",
        "ticker_counts = (\n",
        "    news_df[\"ticker\"]\n",
        "    .value_counts()\n",
        "    .head(10)   # Top-N tickers\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6.5, 4))\n",
        "plt.barh(\n",
        "    ticker_counts.index[::-1],\n",
        "    ticker_counts.values[::-1],\n",
        "    color=\"#6EC6FF\"\n",
        ")\n",
        "\n",
        "plt.title(\"FNSPID News Coverage by Ticker (Top 10)\")\n",
        "plt.xlabel(\"Number of News Articles\")\n",
        "plt.ylabel(\"Ticker\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/fnspid_news_by_ticker_top10.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zn9oF-QXEimY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}